---
title: "최신컴퓨터특강"
date: 2023-04-12
tags: []
---

## 1주차 - 원유재 교수님 - 첨단분야 혁신융합대학사업 (COSS: Convergence and Open Sharing System)
- 추진배경 : 디지털 전환, 글로벌 경쟁 심화에 따라 신기술 초격차 확보 및 급증하는 신산업 인력 수요에 대응할 수 있는 인재 양성
- 충남대학교 : 블록체인 분야 특화, 연구협력(연구단지) 중심 특화
 
### 5개 융복합창의전공
  - 블록체인융합전공
  - 사이버보안융합전공
  - 클라우드융합전공
  - 데이터보안활용융합전공 : 컴퓨터 비전공자 대상
  - 개인정보보호융합전공
 
### 25-1학기 충남대학교 개설 과목
  - 블록체인 개론
  - 블록체인 프로그래밍
  - 디지털 자산과 블록체인

### 학생지원
- 혁신융합대학 학위 취득
- 장학금 지원
- 전문가와 교류 기회
- 학습기기 지원
- 현장실습 기회

## 3주차 - 김형신 교수님

### AI기술의 임베디드 시스템화

#### 이유
- Privacy (개인정보 보호)
- Latency (지연시간)
- Cost (비용)

#### Embedding Resnet
> Resnet을 임베디드 시스템에 적용함

- Resnet : 합성곱 신경망 모델

### 자율주행 자동차에서의 딥러닝 구현

### Image Classification

### DNN and MAC operation
- DNN : Deep Neural Network
- MAC : Multiply and Accumulate


### 임베디드 AI 하드웨어
#### Qualcomm Hexagon DSP
- Performance with low power
- 고속 MAC 기반 프로세서

#### Apple Neural Engine
- 뉴럴넷 가속기와 MMA 기반 병렬처리형 GPU 포함

#### Nvidia Jetson
- 다수의 PE 기반 MMA 가속형 병렬처리 GPU
- 양자화, 프루닝 하드웨어 지원
- 최적화 SW 스택 지원
- Nvidia Xavier NX 컴퓨터
  > Nvidia Jetson 시리즈 중 하나
     
#### Tensor Processing Unit (TPU)
- Google 사에서 개발한 뉴럴프로세서
- 검색엔진, 알파고에 사용

#### Google TPU : High-level Architecture

#### FPGA기반 가속기 (Accelerator)
- 도메인과 응용에 특화된 가속회로를 FPGA로 구현하고 소프트웨어 스택을 제공

#### Microcontrollers(MCU)
- 제어용 임베디드 프로세서로 연산 성능이 매우 낮으며, AI를 위한 가속기능은 없지만, 모델 경량화, 압축을 통해 최적화를 적용
  
### 실시간 온보드 AI 컴퓨팅

### CESL의 연구
- 프레임워크 없이 DNN 추론 가능하게 하기
  - PyTorch, Tensorflow 없이
  - 모든 추론 루틴을 C로 구현
- 실시간 추론
  - 메모리 계층을 활용하는 수학 라이브러리 최적화
  - CPU, GPU, DSP, NPU 최적 워크로드 할당을 통한 계산 가속
- NN/LLM 압축
  - 속도 향상
  - 정확도 손실 없음
- 사용하는 프레임워크
  - TensorRT, ONNX RT
  - Triton, GEMM, llama.cpp
  - Darknet, OpenCL

## 4주차 - 조승범 교수님

### 이론 전산학
> 수학을 이용하여 다양한 전산학 관련 문제들을 정의하고 이를 해결하는 학문

### 1. Integer Sequence Representation
> 알파벳 size가 |A|이고 길이가 n인 integer sequence를 가능한 작은 공간에 저장하기, 단, 일반적인 array 처럼 random acess가 가능해야 함

- 방법 1: 각 alphabet 마다 4비트씩 할당함 -> 4n bits, O(1) access time
- 방법 2: 가능한 모든 10^n개의 sequence에 대해 순서대로 숫자를 부여함
    $$ \left\lceil \log_2(10^n) \right\rceil = \left\lceil n \cdot \log_2 10 \right\rceil \approx \left\lceil 3.32n \right\rceil \text{ bits} $$
    - O(n) access time

### 2. Edit Distance
- edit distance : 두 문자열을 같게 만들기 위해 필요한 최소한의 연산 횟수
- 연산 종류
  - 삽입 (Insert)
  - 삭제 (Delete)
  - 대체 (Replace)
- SETH가 거짓이 아니면 $O(n^2)$ 보다 빨리 계산할 수 없음
- SETH(Strong Exponential Time Hypothesis) : 강한 지수 시간 가설

### 3. Optimal online binary search tree
> 몰루

## 4주차 - 권진근 교수님 - RNN/LSTM

### RNN (Recurrent Neural Network)

$$ h_t = f(Vx_{t} + Ah_{t-1}) $$
- f: tanh
- $x_t$: 현재 입력
- $h_{t-1}$: 과거정보

### LSTM (Long Short Term Memory)
> 이전의 입력 정보가 오래 기억되도록 구조 개선

### Attention Network
> 디코더에서 출력 단어를 예상하는 매 time step마다, 인코더에서의 전체 인풋을 한 번 더 참고

### Transformer
> RNN없이 Attention만으로 Encoder와 Decoder를 설계

- 기본 구조 : 다층 구조인 N개의 encoder layer와 N개의 decoder layer로 구성
- 포지셔널 인코딩 (Positional Encoding)
  - RNN은 단어의 위치에 따라 순차적으로 입력을 받아 각 단어의 위치 정보를 가짐
  - 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니기에 단어의 위치 정보가 필요

- 순환을 모두 없애고 self-attention이라는 특별한 형태의 attention에 의지

#### self-attention
> Attention을 자기자신에게 수행
- input data 안에 있는 벡터들 간의 similarities를 계산

### BERT (Bidirectional Encoder Representations from Transformers)
- Encoder만 사용

### BART (Bidirectional and Auto-Regressive Transformers)
- Encoder와 Decoder 모두 사용

### GPT (Generative pre-trained transformer)
- Decoder만 사용

#### ICL (In-context Learning)
> 모델이 주어진 예시를 보고 학습하는 것

- ICL은 LM이 패턴의 추상화 능력을 가지고 있다는 것을 보여줌
- 종류
  - Zero-shot : 예시 없음
  - One-shot : 예시 하나
  - Few-shot : 예시 여러 개

- Zero-shot chain-of-thought (생각의 사슬)
  > "Let's think step by step"을 prompt에 삽입 -> GPT의 추론을 유도

### RAG (Retrieval-Augmented Generation)
> 검색 증강 생성

### Open Model vs Closed Model
- Open Model : Llama
- Closed Model : ChatGPT

## 5주차 - 김형기 교수님 - 컴퓨터 그래픽스 및 혼합현실 기술 소개 및 현황

## 컴퓨터 그래픽스
### 렌더링
> 데이터로부터 이미지를 만들어내는 기술
- On-line Rendering : 실시간(>30fps)으로 이미지를 만들어내는 기술
- Off-line Rendering : 비실시간으로 이미지를 만들어내는 기술

### On-line Rendering: Rasterization
- 컴퓨터 그래픽에서, 모든 물체는 삼각형의 집합으로 표현됨
- 이러한 삼각형들을 일련의 과정을 거쳐 화면상 픽셀로 변환하는 방식
- Vertex Shader : 어디에 삼각형이 있을 것인가?
- Fragment Shader : 삼각형 안의 픽셀을 어떤 색으로 칠할 것인가?

- 순서 : vertex shader -> rasterization -> fragment shader -> output merger

#### GPU(Graphics Processing Unit)
- 간단한 계산을 할 수 있는 처리 장치 수천-수만개가 동시에 동시에 셰이더를 실행

### Off-line Rendering: Ray/Path Tracing

#### Ray/Path Tracing
> 빛과 물체의 상호작용을 Rasterization 상세히 계산
- 눈으로 들어올 광선을 역으로 추적하여 색상을 계산

#### RTX Hybrid Rendering
> Rasterization + Ray Tracing, 별도의 코어 사용, 딥러닝 기반 기술 적용

## 혼합현실




